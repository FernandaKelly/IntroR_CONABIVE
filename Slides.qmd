---
title: "Introdução a linguagem de programação R e Suas Funcionalidades"
author: "Fernanda Kelly R. Silva | Estatística"
format: 
  revealjs:
    footer: "Slides por [Fernanda Kelly](https://fernandakellyrs.com). Slides [on GitHub](https://github.com/FernandaKelly/IntroR_CONABIVE)."
    slide-number: true
    preview-links: true
    chalkboard: true
    code-annotations: hover
    echo: true
    message: false
    warning: false
editor: visual
---

# Quem é a Fê?

Eu sou uma mulher preta com **coragem** e que fez (e ainda faz) uma boa limonada com os limões que a carreira na área da tecnologia oferece a todas minorias sociais.

# E faz o quê?

## Carreira Acadêmica

::: fragment
-   Graduada em Estatística pela Universidade Federal de Goiás (UFG);
:::

::: fragment
-   Especialista em Data Science e Analytics pela Universidade de São Paulo (USP);
:::

::: fragment
-   Especialista em Gerenciamento de Projetos pela Fundação Getúlio Vargas (FGV);
:::

::: fragment
-   Mestranda em Sistemas Inteligentes e Suas Aplicações na área da Saúde pela Universidade Federal em Ciências da Saúde de Porto Alegre (UFCSPA);
:::

# E sabe o que é o mais íncrivel de TUDO isso?

#  {background-color="#B048A2"}

É que caminho ao lado de grandes mulheres.

![](img/turma_RLadiesgyn.jpg)

::: footer
Curso Introdução ao RBio e Experimentos ministrado por Dthenifer
:::

# 

Sou fundadora de um capítulo da organização mundial R-Ladies. Este capítulo é o [R-Ladies Goiânia](https://www.rladiesgyn.com/) e também faço parte do time global da organização [AI Inclusive](https://www.ai-inclusive.org/).

# E vocês?

# Vamos começar...

# Instalando o R (ou não?)

::: callout-important
Aqui é importante dizer que o nome da linguagem é R e o nome da IDE (integrated development environment) é RStudio, ou seja, a IDE RStudio é um ambiente de desenvolvimento que utilizamos para editar e executar os códigos em R. E claro, não existe somente a IDE RStudio que oferece a possibilidade de trabalhar com a linguagem R, há várias outras que também executam códigos em R, por exemplo: **Visual Studio Code (VSCode)**.
:::

## As opções...

Você tem duas opções para trabalhar com a IDE RStudio:

# Posit Cloud

::: fragment
Ou antigo [RStudio Cloud](https://www.fernandakellyrs.com/post/rstudio-cloud)
:::

::: fragment
-   Para acessar o [Posit Cloud](https://posit.cloud/) você pode fazer login com o gmail, GitHub ou qualquer outra conta que você tiver interesse em utilizar para o cadastro;
:::

::: fragment
-   Você poderá utilizar o espaço referente a 25 projetos e 25 horas (por mês) de computação. As horas de computação representam o uso da máquina virtual durante cada período para editar, executar, renderizar ou apresentar conteúdo.
:::

# 

-   Por padrão, cada projeto recebe 1 GB de RAM e 1 CPU, além de um limite de execução em segundo plano de 1 hora.

# Localmente

::: fragment
Ou seja, na sua máquina.
:::

::: fragment
-   No dia a dia, utilizar o R localmente é mais funcional e, para isso, você deve [instalar o R](https://posit.co/download/rstudio-desktop/);
-   Em relação a IDE RStudio você pode instalar [aqui](https://posit.co/download/rstudio-desktop/).
:::

# E porquê usar o R?

# 

-   É uma linguagem de programação que possui muitas ferramentas para análise de dados, visualização de dados e tantas outras ferramentas;

-   É código aberto (open source);

-   Possui uma comunidade ativa de pessoas desenvolvedoras ([curso-r fórum](https://discourse.curso-r.com/));

-   É flexível, permite desenvolver funções e pacotes para facilitar o trabalho;

-   Está disponível, gratuitamente, em diferentes sistemas operacionais: Windows, Linux e Mac.

# 

-   Os pacotes R contêm **código**, **dados** e **documentação** em um formato de coleção padronizado que pode ser instalado pelos usuários do R, normalmente por meio de um repositório de software centralizado como o **CRAN (Comprehensive R Archive Network)**.
-   O grande número de pacotes disponíveis para o R e a facilidade de instalação e uso deles são citados como um fator importante que impulsiona a adoção generalizada da linguagem na ciência de dados.

# 

Mas, por onde começar Fê?

# O início de tudo...

# ... são as boas práticas!

# 

O ponto é: **vale mais a pena se esforçar para encontrar um par de meias no meio de uma gaveta bagunçada ou vale mais a pena se esforçar para arrumar uma gaveta bagunçada?**

# Step 1 {background-color="#FDE962"}

# 

Essa etapa do curso vai ser de muita troca e **live coding**. Vamos aprender as funcionalidades do R juntinhos e operantes, bora?

# Step 2 {background-color="#FDE962"}

#  {background-image="img/tidyverse-default.png"}

# 

O universo **Tidyverse** é basicamente uma coleção de pacotes voltados a ciência de dados.

-   Estes pacotes tem filosofia, gramática e estruturas de dados em comum, ou seja, foram criadas para um pacote poder trabalhar em conjunto com o outro.

#  {background-color="black"}

Os pacotes que fazem parte desse universo são:

![](img/tidyverse_pacotes.jpeg)

#  {background-image="img/tidyverse_forma.png"}

# Dplyr

## Funções que vamos aprender

O pacote auxilia na manipulação de dados, realizando-a de forma eficiente e, com essas funções, temos uma flexibilidade incrível para transformar nossos dados.

-   rename(); distinct();

-   select(); filter();

-   mutate(); transmute();

-   arrange(); summarise();

-   group_by(); sample_frac();

-   sample_n().

# Mas antes de tudo...

# ... precisamos de uma base de dados!

# Base de dados

É importante dizer que há várias formas da gente conseguir encontrar bases de dados são várias, inclusive por meio de pacotes. Te convido a conhecer o pacote [**basedosdados**](https://basedosdados.org/) e desbravar as várias bases de dados disponíveis, mas hoje, nós vamos utilizar o banco de dados **CO2** que é do pacote **datasets**.

```{r}
library(datasets)
```

## Carbon Dioxide Uptake in Grass Plants

A absorção de $CO_{2}$ por seis plantas de Quebec e seis plantas do Mississippi foi medida em vários níveis de concentração ambiental. Metade das plantas de cada tipo foram resfriadas durante a noite antes da realização do experimento. Este conjunto de dados fazia originalmente parte do pacote nlme e possui métodos para suas classes de dados agrupados.

```{r}
dados_Carbon <- datasets::CO2
head(dados_Carbon,3)
```

## 

As variáveis que compõem o banco de dados são:

-   Plant: um fator ordenado com níveis Qn1 \< Qn2 \< Qn3 \< ... \< Mc1 fornecendo um identificador único para cada planta.

-   Type: um fator com dois níveis, sendo eles **Quebec** e **Mississippi** dando a origem da planta.

-   Treatment: um fator com dois níveis, sendo eles **nonchilled** e **chilled**.

-   conc: um vetor numérico de concentrações ambientais de dióxido de carbono (mL/L).

-   uptake: um vetor numérico de taxas de absorção de dióxido de carbono ($\mu mol/m^{2} sec$).

# Vamos aprender manipulação de dados?

#  {background-image="img/featured.png"}

# 

Pipes são uma ferramenta poderosa para expressar claramente uma sequência de múltiplas operações. A versão 4.0+ do R possui um pipe “nativo” **\|\>** que não depende da dependência do pacote magrittr. Aparentemente, ele deveria ser mais rápido que o pipe do pacote magrittr, por fazer muito mais “nos bastidores” do que o **%\>%**. Você pode ler mais sobre isso em [R for Data Science](https://r4ds.had.co.nz/pipes.html).

```{r}
library(magrittr)
```

# 

Exemplo:

```{r}
x <- c(1.234, 2.4536, 3.442, 3.24, 1.2223)

sum(round(mean(x),2),5)

x %>% mean() %>% round(2) %>% sum(5)
```

------------------------------------------------------------------------

# rename()

# 

A primeira função que vamos estudar é a *rename()*. Esta função irá *renomear* as variáveis do banco de dados. Vejamos o exemplo a seguir:

```{r}
dados_Carbon %>% 
  dplyr::rename("planta" = "Plant") %>% 
  utils::head(3)
```

# 

Nós temos também a opção da função **rename_with**, em que podemos aplicar funções de interesse para renomear as variáveis.

```{r}
dados_Carbon %>% 
  dplyr::rename_with(toupper) %>%
  #dplyr::rename_with(tolower) %>%
  utils::head(3)
```

# 

Há também as seguintes variações:

-   rename_if

```{r}
dados_Carbon %>% 
  dplyr::rename_if(is.factor, toupper) %>% 
  utils::head(1)
```

-   rename_at

```{r eval=FALSE}
dados_Carbon %>% 
  dplyr::rename_at(vars(Plant:conc), toupper) %>% 
  utils::head(1)
```

# 

-   rename_all

```{r}
dados_Carbon %>% 
  dplyr::rename_all(toupper) %>% 
  utils::head(1)
```

# distinct()

# 

A função **distinct()** seleciona apenas as linhas únicas (distintas) de uma determinada variável do banco de dados.

```{r}
dados_Carbon %>% 
  dplyr::distinct(Type)
```

# select()

# 

A função **select()** permite selecionar colunas nomeando-as e separando-as por vírgula ou não. Veremos exemplos abaixo.

```{r}
dados_Carbon %>% 
  dplyr::select(1,3) %>% 
  utils::head(3)
```

Veja que com o código abaixo a saída é a mesma:

```{r eval = FALSE}
dados_Carbon %>%
  dplyr::select(Plant, Treatment) %>% 
  utils::head(3)
```

# 

Você pode selecionar colunas utilizando os dois pontos **:**, mas nesse caso, a seleção das colunas seguem a ideia de continuação. Por semplo:

```{r}
dados_Carbon %>% 
  dplyr::select(Plant:Treatment) %>% 
  utils::head(3)
```

# 

Exisem funções no `dplyr` que ajudam na seleção das variáveis de uma outra forma:

-   starts_with(): colunas que começam com um prefixo
-   ends_with(): colunas que terminam com um sufixo
-   contains(): colunas que contêm uma string
-   last_col(): última coluna

# 

```{r}
dados_Carbon %>% 
  dplyr::select(starts_with("Pl")) %>% 
  utils::head(3)
```

# 

Esta função nos permite retirar colunas da base da seguinte forma:

```{r}
dados_Carbon %>% 
  dplyr::select(-Treatment) %>% 
  utils::head(3)
```

**A função select() também possui as variações select_if(), select_at() e select_all().**

# filter()

# 

Já a função **filter()** permite subdividir observações com base em seus valores. O primeiro argumento é o nome do quadro de dados. O segundo argumento e os subsequentes são as expressões que filtram o quadro de dados.

```{r include=FALSE}
dados_Carbon %>% 
  dplyr::filter(Treatment == "nonchilled") %>% 
  utils::head(3)
```

Quando você executa essa linha de código, o pacote `dplyr` executa a operação de filtragem e retorna um novo quadro de dados.

```{r}
dplyr::filter(dados_Carbon, Treatment == "nonchilled") %>% 
  utils::head(3)
```

# 

Você pode também utilizar essa função de forma eficaz, você precisa saber como selecionar as observações desejadas usando os operadores de comparação. O R fornece o conjunto padrão: \>, \>=, \<, \<=, != (diferente) e == (igual).

```{r}
dplyr::filter(dados_Carbon, conc <= 95) %>% 
  utils::head(3)
```

# 

Vários argumentos do **filter()** são combinados com **“e”**: cada expressão deve ser verdadeira para que uma linha seja incluída na saída. Para outros tipos de combinações, você mesmo precisará usar operadores booleanos: & é “e”, \| é “ou” e ! é “não”. A Figura abaixo mostra o conjunto completo de operações booleanas.

![](img/boolenaos.jpeg)

# 

No exemplo a seguir utilizamos o operador **\<=** (menor igual) e o **&**:

```{r}
dados_Carbon %>% 
  dplyr::filter(conc <= 95 & Treatment == "nonchilled") %>% 
  utils::head(3)
```

**A função filter() também possui as variações filter_if(), filter_at() e filter_all().**

# mutate()

# 

Além de selecionar conjuntos de colunas existentes, geralmente é útil adicionar novas colunas que sejam funções de colunas existentes. Esse é o trabalho da função **mutate()**.

**mutate()** sempre adiciona novas colunas no final do seu conjunto de dados, então começaremos criando um conjunto de dados mais restrito para que possamos ver as novas variáveis. Lembre-se que quando você estiver na IDE RStudio, a maneira mais fácil de ver todas as colunas é através da função **View()**.

```{r}
exemplo_mutate <- dados_Carbon %>% 
                    dplyr::mutate(soma_numeric = conc + uptake,
                                  plant_trat = stringr::str_c(Plant, Treatment, sep = "-")) 
```

# 

Observe que você pode consultar colunas que acabou de criar:

```{r}
dados_Carbon %>% 
 dplyr::mutate(soma_numeric = conc + uptake) %>% 
 utils::head(3)
```

# transmute()

# 

Se você quiser apenas manter as novas variáveis, use a função **transmute()**:

```{r}
exemplo_transmute <- dados_Carbon %>% 
                       dplyr::transmute(soma_numeric = conc + uptake,
                                     plant_trat = stringr::str_c(Plant, Treatment, sep = "-")) %>% 
                       utils::head(3)
```

# arrange()

# 

A função **arrange()** orderna linhas.

-   Se a variável for categórica ordena por ordem alfabética
-   Se a variável for numérica ordena do menor para o maior
-   Se a varíavel for um fator, ordena pelos níveis do fator

# 

O exemplo abaixo ordena o banco de dados de acordo com a variável **uptake**.

```{r}
dados_Carbon %>%
  dplyr::arrange(uptake) %>% 
  utils::head(3)
```

# 

Mas, nós podemos ordenar as linhas de forma decrescente usando a função **desc()**. Veja o exemplo abaixo.

```{r}
dados_Carbon %>% 
  dplyr::arrange(desc(uptake)) %>% 
  utils::head(3)
```

# 

É também possível ordenar mais de uma coluna de uma vez

```{r}
dados_Carbon %>%
  dplyr::arrange(uptake, conc) %>% 
  utils::head(3)
```

# group_by()

# 

Temos a função **group_by()** que irá agrupar o banco de dados em relação a uma variável (ou mais de uma).

```{r}
dados_Carbon %>% 
  dplyr::group_by(Treatment) 
```

#  {background-color="black"}

Quando dados agrupados são passados para uma métrica com reconhecimento de grupo, eles retornarão valores de métrica calculados para cada grupo.

![](img/group.jpeg)

# 

Logo, se temos 2 tratamentos, e se quiséssemos calcular a média de cada grupo em relação a variável **conc**, precisaríamos que os dados estivessem agrupados de acordo com o tratamento.

::: fragment
**A função group_by() é comumente utilizada com a função summarise() que vem logo a seguir**
:::

# summarise()

# 

O último verbo chave é o **summarise()**. Ele recolhe um quadro de dados em uma única linha.

A função **summarise()** não é muito útil, a menos que o combinemos com **group_by()**. Isso altera a unidade de análise do conjunto de dados completo para grupos individuais. Então, quando você usar as funções `dplyr` em um quadro de dados agrupado, eles serão aplicados automaticamente “por grupo”.

# 

![](img/summarize-a-variable-in-R.png)

# 

Por exemplo, se aplicarmos exatamente o mesmo código a um quadro de dados agrupado por tratamento, obteremos o valor de **uptake** médio por tratamento:

```{r}
dados_Carbon %>% 
  dplyr::group_by(Treatment) %>% 
  dplyr::summarise(media_trat = base::mean(uptake, na.rm = TRUE),
                   n_plantas = dplyr::n())
```

::: fragment
**Juntos group_by() e summarise() fornecem uma das ferramentas que você usará mais comumente ao trabalhar com `dplyr`: resumos agrupados.**
:::

# case_when()

# 

O **case_when** é uma generalização do **if_else()**. Cada caso é avaliado sequencialmente e a primeira correspondência de cada elemento determina o valor correspondente no vetor de saída. Se nenhum caso corresponder, o **.default** será usado como uma instrução final "else".

-   case_when() é um equivalente em R da CASE WHEN instrução `SQL` "pesquisada".

# 

```{r}
dados_Carbon %>% 
  dplyr::mutate(
    new_plant = dplyr::case_when(conc >= 125 ~ "Tipo1",
                                 .default = "Tipo2")
    ) %>% 
  utils::head(1)
```

# across()

**across()** facilita a aplicação da mesma transformação a múltiplas colunas, permitindo que você use a semântica **select()** dentro de funções de "mascaramento de dados" como **summarise()** e **mutate()**.

**if_any()** e **if_all()** aplicam a mesma função de predicado a uma seleção de colunas e combinam os resultados em um único vetor lógico.

# 

-   if_any() é TRUE quando o predicado é TRUE para qualquer uma das colunas selecionadas
-   if_all() é TRUE quando o predicado é TRUE para todas as colunas selecionadas

Se você só precisa selecionar colunas sem aplicar uma transformação a cada uma delas, provavelmente desejará usar **pick()**.

::: fragment
**across() substitui a família de "variantes com escopo definido" como summarise_at(), summarise_if() e summarise_all().**
:::

# 

Um exemplo bem rapidinho:

```{r}
dados_Carbon%>%   
  dplyr::group_by(Treatment) %>%   
  dplyr::summarise(dplyr::across(c(conc,uptake), mean, na.rm = TRUE))
```

::: fragment
Deixo o post no meu blog sobre [a função across](https://www.fernandakellyrs.com/post/fun%C3%A7%C3%A3o-across) para vocês estudarem mais sobre ela.
:::

#  {background-image="img/esquisse.jpeg"}

# 

O esquisse é um pacote que permite a criação de gráficos em {ggplot2} de maneira point and click, o que torna a construção de gráficos uma tarefa bem mais simples, pois não há necessidade de digitar as linhas de código.

Além disso, é possível recuperar o código que gerou os gráficos, o que é ótimo para garantir a reprodutibilidade dos resultados!

# Legal, né?

# 

Vamos brincar um pouco com essa aplicação.

```{r eval=FALSE}
library(esquisse)
esquisse::esquisser(viewer = "browser")
```


#  {background-image="img/teste_hipo.png"}

#
**A parte final de uma análise estatı́stica, o teste de hipotese (T.H.), se dá pela tomada de decisão relativa a um problema de interesse, digamos, verificar se há associação entre um fator (adubo) e o tamanho de uma planta (mangueira) na experimentação agronômica.**

# 

Na prática experimental é comum o surgimento de situações em que não se tem o conhecimento da distribuição dos dados, ou mesmo que o tenha, os mesmos podem não ter a distribuição normal. A alta dispersão ou até mesmo assimetria, ou ambos, fazem com que o trato com os dados seja dificultado.

::: {.notes}
-   **Muitos casos destes existem técnicas de transformações nos dados que produzem novos dados cuja distribuição se aproxime da normal**. Caso isto não seja possı́vel as técnicas não-paramétricas são uma alternativa para análise estatı́stica.
:::

# Paramétricos

Há três principais tipos de testes t: o teste t para uma amostra, o teste t para duas amostras independentes (ou apenas teste t independente) e o teste t para duas amostras pareadas (ou teste t pareado). O teste t é um teste que verifica se há **diferença média** entre as amostras.

::: fragment
E se você tiver mais de duas amostras ou grupos?
:::

# 

A resposta mais inocente seria realizar múltiplos testes t entre os diversos grupos e comparar o tamanho de efeito e p-valor dos testes. Porém essa abordagem possui uma falha: conforme aumenta o número de testes a taxa de falsos positivos (erros tipo I) aumentam quase na mesma proporção. **Esse erro ocorre quando rejeitamos a hipótese nula quando de fato ela é verdadeira.** 

# 

Para comparação entre mais de duas amostras ou grupos, temos um conjunto de técnicas chamada análise de variância, conhecida como **Analysis of Variance (ANOVA)**.

A ANOVA controla esses erros para que os falsos positivos (erros tipo I) permaneçam em 5% na comparação de média entre dois ou mais grupos. **A ANOVA não irá dizer com precisão como que as médias dos grupos diferem**, mas o seu resultado indica fortes evidências de que a diferença entre as médias dos grupos difere. Sua hipótese nula é que não há diferença entre as médias dos grupos.

# Vamos de exemplo?

# 

Nós vimos que há uma suposição entre a média do vetor numérico de taxas de absorção de dióxido de carbono se difere entre os tratamentos. Sendo assim, vamos levantar a seguinte hipótese entre os grupos nonchilled e chilled:

- $H_{0}$ : Há uma diferença média entreos grupos nonchilled e chilled em relação as taxas de absorção de dióxido de carbono

## Pressupostos

- Normalidade: verificamos a normalidade da variável dependente de cada grupo;
- Homogeneidade de variâncias (ou seja, grupos com variâncias homogêneas).

## Normalidade

Para verificar a normalidade dos dados, é recomendável utilizar o teste de **Shapiro WIlk** quando o número de amostra é menor que 50. A estatı́stica T é basicamente o quadrado de um coeficiente de correlação, onde o coeficiente de correlação de Pearson é calculado entre a estatı́stica ordenada X i na amostra e o escore ai , que representa o que a estatı́stica ordenada deveria parecer se a população é Normal. Portanto, as hipóteses levantadas são:

- $H_{0}$ : F(x) é função de distribuição normal;
- $H_{1}$ : F(x) não é função de distribuição normal;

#

Iremos fixar o nível de 95% de significância.

A estatística de teste já foi mencionada acima, mas vale frizar que vamos utilizar o teste de **Shapiro WIlk** .

```{r}
#| message: false
#| warning: false

library(rstatix)

teste_shapiro <- dados_Carbon %>% 
                  dplyr::group_by(Treatment) %>% 
                  rstatix::shapiro_test(uptake)

knitr::kable(teste_shapiro)
```

#

Veja que de acordo com o teste de Shapiro Wilk, ao nível de 95% de significância, **rejeita-se a hipótese nula**, ou seja, a variável **uptake** não é descrita por uma distribuição normal.

# E agora, faço o quê?

# Transformação de Box-Cox

Em estatística, uma transformação de potência é uma família de funções que são aplicadas para criar a transformação monotônica de dados usando funções de potência. Esta é uma técnica de transformação de dados útil usada para estabilizar a variância, tornar os dados mais semelhantes à distribuição normal, melhorar a validade das medidas de associação (como a correlação de Pearson entre as variáveis) e para outros procedimentos de estabilização de dados.

A transformação de Box-Cox recebeu o nome dos estatísticos que a formularam, George E. P. Box y David Cox, em artigo de 1964 (“An Analysis of Transformations”).

#

Essa transformação consiste em transformar os dados de aordo com a seguinte expressão:

- $y' = \frac{y^\lambda - 1}{\lambda}$

onde $\lambda$ é um parâmetro a ser estimado dos dados. Se $\lambda = 0$, temos:

- $y' = ln(y)$

**Então a nossa missão é encontrar o valor de $\lambda$.**

#

Nós vamos utilizar o pacote `MASS` para obter o valor de interesse para executar a transformação. 

```{r}
library(MASS)
```

```{r}
lambda <- MASS::boxcox(aov(uptake+0.000001~Treatment,  data = dados_Carbon ))
lambda$x[which.max(lambda$y)]
```

Desta forma o próximo passo é obter os dados transformados e depois realizar as análises utilizando estes novos dados.

#

Nós aprendemos a função **mutate** e agora vamos utilizá-la para criar essa nova variável.

```{r}
dados_Carbon <- dados_Carbon %>% 
                  dplyr::mutate(new_uptake = ((uptake^0.8282828) - 1)/0.8282828)
```

::: fragment
Vamos plotar essa nova variável?
:::


#







## Homogeneidade de variâncias

O segundo pressuposto pode ser avaliado através do  **teste de Levene**, em que as hipóteses são:

- $H_{0}$ : Os grupos apresentam variâncias homogêneas;
- $H_{1}$ : Os grupos não apresentam variâncias homogêneas;

Vamos considerar o nível de 95% de significância.

#

```{r}
#| warning: false
#| message: false

library(car)
teste_levene <- dados_iris %>% 
                  dplyr::filter(Species != "versicolor") %>% 
                  dplyr::group_by(Species) 

teste_levene_Resultado <- car::leveneTest(Sepal.Length ~ Species,
                                          data = teste_levene, 
                                          center = mean)
knitr::kable(teste_levene_Resultado)
```

#

Logo, ao nível de 95% de significância, **rejeitamos a hipótese nula** (p-value = 0.0003), ou seja, os grupos apresentam não variâncias homogêneas e assim não podemos seguir com o teste t independente.

::: fragment
**Violamos o pressuposto de variâncias homogêneas, e agora?**
:::

::: fragment
Nós poderemos seguir com o teste-t, mas deveremos usar um teste-t com correção para essa violação da homogeneidade, a **correção de Welch**.
:::

# Partiu teste, Fê?

# ...se liga só no teste e em suas miniciosidades.

#

Como violamos o teste de homogeinidade de variâncias, vamos programar com **var.equal=FALSE**, mas uma vez que o pressuposto da homogeneidade de variâncias foi atendido, colocaremos **var.equal=TRUE**.

::: fragment
```{r}
#| message: false
#| warning: false

stats::t.test(Sepal.Length ~ Species,
              data = teste_levene,
              var.equal = FALSE)
```

:::

#

Qual é a interpretação dessa saída?

::: fragment
**Ao nível de 95% de significância, nós rejeitamos a $H_{0}$ e assim há diferença média entre as Species e o tamanho da Sepal.Length.**
:::

::: fragment
Se o nosso interesse for comparar os três tipos de Species, devemos utilizar a ANOVA e, para um melhor entendimento da diferença, utilizariamos o teste de Tukey para apontar quais grupos são diferentes entre si.
:::

#

Mas antes de tudo...

# Teste t

## paired = TRUE

Em algumas situações o par de amostras é constituı́do do mesmo indivı́duo em duas ocasiões diferentes. Essa ocasiação nós denominamos por **pareamento**, em que neste pareamento podemos distingui-lo em três tipos: auto-pareamento, pareamento natural e pareamento artificial.

#

- O auto-pareamento ocorre quando o indivı́duo serve como seu próprio controle, como na situação
em que um indivı́duo recebe duas drogas administradas em ocasiões diferentes. Outra situação é
a que um tratamento é administrado e as variáveis de interesse são observadas antes e depois do
programa. Finalmente, a comparação de dois órgãos no mesmo indivı́duo, como braços, pernas,
olhos, narinas, segundo alguma caracterı́stica estudada também constitui um auto-pareamento.

#

- O pareamento natural consiste em formar pares tão homogêneos quanto possı́vel, controlando
os fatores que possam interferir na resposta, sendo que o pareamento aparece de forma natural. Por
exemplo, em experimentos de laboratório pode-se formar pares de cobaias selecionadas da mesma
ninhada; em investigações clı́nicas, gêmeos univitelinos são muito usados.

- No pareamento artificial escolhe-se indivı́duos com caracterı́sticas semelhantes, tais como, idade,
sexo, nı́vel sócio-econômico, estado de saúde ou, em geral, fatores que podem influenciar de maneira
relevante a variável resposta.

#

```r
t.test(x, ...)

## Default S3 method:
t.test(x, y = NULL,
       alternative = c("two.sided", "less", "greater"),
       mu = 0,
       **paired = FALSE**,
       var.equal = FALSE,
       conf.level = 0.95, ...)

## S3 method for class 'formula'
t.test(formula, data, subset, na.action, ...)
```

## alternative = c("two.sided", "less", "greater")

A hipótese alternativa mais comum - e a mais recomendada, salvo raras exceções - é a de que há uma diferença entre os grupos. Observe que aqui estamos estabelecendo apenas que, caso os grupos não sejam iguais, eles serão diferentes. Não estamos nos comprometendo com o sentido dessa diferença, se o grupo A terá uma média superior ou inferior à do grupo B. No entanto, é possível construirmos hipóteses alternativas que estabeleçam o sentido dessa diferença:

#

- $H_{1}$ : média do grupo A > média do grupo B
- $H_{1}$ : média do grupo A < média do grupo B


![](img/bilateral.png)

::: footer
Arte por [Allison Horst](https://mobile.twitter.com/allison_horst)
:::

#

```r
t.test(x, ...)

## Default S3 method:
t.test(x, y = NULL,
       **alternative = c("two.sided", "less", "greater")**,
       mu = 0,
       paired = FALSE,
       var.equal = FALSE,
       conf.level = 0.95, ...)

## S3 method for class 'formula'
t.test(formula, data, subset, na.action, ...)
```

# ANOVA

# ... bem rapidinho, pode ser?

#

::: fragment
```{r}
anova <- stats::aov(Sepal.Length ~ Species, dados_iris)
summary(anova)
```
:::

::: fragment
Qual é a interpretação desse teste?
:::

#

::: fragment
```{r}
stats::TukeyHSD(anova)
```
:::

::: fragment
Onde há diferença?
:::


# K-Means
