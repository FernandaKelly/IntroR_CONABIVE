{
  "hash": "3720f14ede1ce1c79c7b4a3913f6450b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introdução a linguagem de programação R e Suas Funcionalidades\"\nauthor: \"Fernanda Kelly R. Silva | Estatística\"\nformat: \n  revealjs:\n    footer: \"Slides por [Fernanda Kelly](https://fernandakellyrs.com). Slides [on GitHub](https://github.com/FernandaKelly/IntroR_CONABIVE).\"\n    slide-number: true\n    preview-links: true\n    chalkboard: true\n    code-annotations: hover\n    echo: true\n    message: false\n    warning: false\neditor: visual\n---\n\n\n\n# Quem é a Fê?\n\nEu sou uma mulher preta com **coragem** e que fez (e ainda faz) uma boa limonada com os limões que a carreira na área da tecnologia oferece a todas minorias sociais.\n\n# E faz o quê?\n\n## Carreira Acadêmica\n\n::: fragment\n-   Graduada em Estatística pela Universidade Federal de Goiás (UFG);\n:::\n\n::: fragment\n-   Especialista em Data Science e Analytics pela Universidade de São Paulo (USP);\n:::\n\n::: fragment\n-   Especialista em Gerenciamento de Projetos pela Fundação Getúlio Vargas (FGV);\n:::\n\n::: fragment\n-   Mestranda em Sistemas Inteligentes e Suas Aplicações na área da Saúde pela Universidade Federal em Ciências da Saúde de Porto Alegre (UFCSPA);\n:::\n\n# E sabe o que é o mais íncrivel de TUDO isso?\n\n#  {background-color=\"#B048A2\"}\n\nÉ que caminho ao lado de grandes mulheres.\n\n![](img/turma_RLadiesgyn.jpg)\n\n::: footer\nCurso Introdução ao RBio e Experimentos ministrado por Dthenifer\n:::\n\n# \n\nSou fundadora de um capítulo da organização mundial R-Ladies. Este capítulo é o [R-Ladies Goiânia](https://www.rladiesgyn.com/) e também faço parte do time global da organização [AI Inclusive](https://www.ai-inclusive.org/).\n\n# E vocês?\n\n# Vamos começar...\n\n# Instalando o R (ou não?)\n\n::: callout-important\nAqui é importante dizer que o nome da linguagem é R e o nome da IDE (integrated development environment) é RStudio, ou seja, a IDE RStudio é um ambiente de desenvolvimento que utilizamos para editar e executar os códigos em R. E claro, não existe somente a IDE RStudio que oferece a possibilidade de trabalhar com a linguagem R, há várias outras que também executam códigos em R, por exemplo: **Visual Studio Code (VSCode)**.\n:::\n\n## As opções...\n\nVocê tem duas opções para trabalhar com a IDE RStudio:\n\n# Posit Cloud\n\n::: fragment\nOu antigo [RStudio Cloud](https://www.fernandakellyrs.com/post/rstudio-cloud)\n:::\n\n::: fragment\n-   Para acessar o [Posit Cloud](https://posit.cloud/) você pode fazer login com o gmail, GitHub ou qualquer outra conta que você tiver interesse em utilizar para o cadastro;\n:::\n\n::: fragment\n-   Você poderá utilizar o espaço referente a 25 projetos e 25 horas (por mês) de computação. As horas de computação representam o uso da máquina virtual durante cada período para editar, executar, renderizar ou apresentar conteúdo.\n:::\n\n# \n\n-   Por padrão, cada projeto recebe 1 GB de RAM e 1 CPU, além de um limite de execução em segundo plano de 1 hora.\n\n# Localmente\n\n::: fragment\nOu seja, na sua máquina.\n:::\n\n::: fragment\n-   No dia a dia, utilizar o R localmente é mais funcional e, para isso, você deve [instalar o R](https://posit.co/download/rstudio-desktop/);\n-   Em relação a IDE RStudio você pode instalar [aqui](https://posit.co/download/rstudio-desktop/).\n:::\n\n# E porquê usar o R?\n\n# \n\n-   É uma linguagem de programação que possui muitas ferramentas para análise de dados, visualização de dados e tantas outras ferramentas;\n\n-   É código aberto (open source);\n\n-   Possui uma comunidade ativa de pessoas desenvolvedoras ([curso-r fórum](https://discourse.curso-r.com/));\n\n-   É flexível, permite desenvolver funções e pacotes para facilitar o trabalho;\n\n-   Está disponível, gratuitamente, em diferentes sistemas operacionais: Windows, Linux e Mac.\n\n# \n\n-   Os pacotes R contêm **código**, **dados** e **documentação** em um formato de coleção padronizado que pode ser instalado pelos usuários do R, normalmente por meio de um repositório de software centralizado como o **CRAN (Comprehensive R Archive Network)**.\n-   O grande número de pacotes disponíveis para o R e a facilidade de instalação e uso deles são citados como um fator importante que impulsiona a adoção generalizada da linguagem na ciência de dados.\n\n# \n\nMas, por onde começar Fê?\n\n# O início de tudo...\n\n# ... são as boas práticas!\n\n# \n\nO ponto é: **vale mais a pena se esforçar para encontrar um par de meias no meio de uma gaveta bagunçada ou vale mais a pena se esforçar para arrumar uma gaveta bagunçada?**\n\n# Step 1 {background-color=\"#FDE962\"}\n\n# \n\nEssa etapa do curso vai ser de muita troca e **live coding**. Vamos aprender as funcionalidades do R juntinhos e operantes, bora?\n\n# Step 2 {background-color=\"#FDE962\"}\n\n#  {background-image=\"img/tidyverse-default.png\"}\n\n# \n\nO universo **Tidyverse** é basicamente uma coleção de pacotes voltados a ciência de dados.\n\n-   Estes pacotes tem filosofia, gramática e estruturas de dados em comum, ou seja, foram criadas para um pacote poder trabalhar em conjunto com o outro.\n\n#  {background-color=\"black\"}\n\nOs pacotes que fazem parte desse universo são:\n\n![](img/tidyverse_pacotes.jpeg)\n\n#  {background-image=\"img/tidyverse_forma.png\"}\n\n# Dplyr\n\n## Funções que vamos aprender\n\nO pacote auxilia na manipulação de dados, realizando-a de forma eficiente e, com essas funções, temos uma flexibilidade incrível para transformar nossos dados.\n\n-   rename(); distinct();\n\n-   select(); filter();\n\n-   mutate(); transmute();\n\n-   arrange(); summarise();\n\n-   group_by(); sample_frac();\n\n-   sample_n().\n\n# Mas antes de tudo...\n\n# ... precisamos de uma base de dados!\n\n# Base de dados\n\nÉ importante dizer que há várias formas da gente conseguir encontrar bases de dados são várias, inclusive por meio de pacotes. Te convido a conhecer o pacote [**basedosdados**](https://basedosdados.org/) e desbravar as várias bases de dados disponíveis, mas hoje, nós vamos utilizar o banco de dados **CO2** que é do pacote **datasets**.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(datasets)\n```\n:::\n\n\n\n## Carbon Dioxide Uptake in Grass Plants\n\nA absorção de $CO_{2}$ por seis plantas de Quebec e seis plantas do Mississippi foi medida em vários níveis de concentração ambiental. Metade das plantas de cada tipo foram resfriadas durante a noite antes da realização do experimento. Este conjunto de dados fazia originalmente parte do pacote nlme e possui métodos para suas classes de dados agrupados.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon <- datasets::CO2\nhead(dados_Carbon,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant   Type  Treatment conc uptake\n1   Qn1 Quebec nonchilled   95   16.0\n2   Qn1 Quebec nonchilled  175   30.4\n3   Qn1 Quebec nonchilled  250   34.8\n```\n\n\n:::\n:::\n\n\n\n## \n\nAs variáveis que compõem o banco de dados são:\n\n-   Plant: um fator ordenado com níveis Qn1 \\< Qn2 \\< Qn3 \\< ... \\< Mc1 fornecendo um identificador único para cada planta.\n\n-   Type: um fator com dois níveis, sendo eles **Quebec** e **Mississippi** dando a origem da planta.\n\n-   Treatment: um fator com dois níveis, sendo eles **nonchilled** e **chilled**.\n\n-   conc: um vetor numérico de concentrações ambientais de dióxido de carbono (mL/L).\n\n-   uptake: um vetor numérico de taxas de absorção de dióxido de carbono ($\\mu mol/m^{2} sec$).\n\n# Vamos aprender manipulação de dados?\n\n#  {background-image=\"img/featured.png\"}\n\n# \n\nPipes são uma ferramenta poderosa para expressar claramente uma sequência de múltiplas operações. A versão 4.0+ do R possui um pipe “nativo” **\\|\\>** que não depende da dependência do pacote magrittr. Aparentemente, ele deveria ser mais rápido que o pipe do pacote magrittr, por fazer muito mais “nos bastidores” do que o **%\\>%**. Você pode ler mais sobre isso em [R for Data Science](https://r4ds.had.co.nz/pipes.html).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr)\n```\n:::\n\n\n\n# \n\nExemplo:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(1.234, 2.4536, 3.442, 3.24, 1.2223)\n\nsum(round(mean(x),2),5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.32\n```\n\n\n:::\n\n```{.r .cell-code}\nx %>% mean() %>% round(2) %>% sum(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.32\n```\n\n\n:::\n:::\n\n\n\n------------------------------------------------------------------------\n\n# rename()\n\n# \n\nA primeira função que vamos estudar é a *rename()*. Esta função irá *renomear* as variáveis do banco de dados. Vejamos o exemplo a seguir:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::rename(\"planta\" = \"Plant\") %>% \n  utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  planta   Type  Treatment conc uptake\n1    Qn1 Quebec nonchilled   95   16.0\n2    Qn1 Quebec nonchilled  175   30.4\n3    Qn1 Quebec nonchilled  250   34.8\n```\n\n\n:::\n:::\n\n\n\n# \n\nNós temos também a opção da função **rename_with**, em que podemos aplicar funções de interesse para renomear as variáveis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::rename_with(toupper) %>%\n  #dplyr::rename_with(tolower) %>%\n  utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  PLANT   TYPE  TREATMENT CONC UPTAKE\n1   Qn1 Quebec nonchilled   95   16.0\n2   Qn1 Quebec nonchilled  175   30.4\n3   Qn1 Quebec nonchilled  250   34.8\n```\n\n\n:::\n:::\n\n\n\n# \n\nHá também as seguintes variações:\n\n-   rename_if\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::rename_if(is.factor, toupper) %>% \n  utils::head(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  PLANT   TYPE  TREATMENT conc uptake\n1   Qn1 Quebec nonchilled   95     16\n```\n\n\n:::\n:::\n\n\n\n-   rename_at\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::rename_at(vars(Plant:conc), toupper) %>% \n  utils::head(1)\n```\n:::\n\n\n\n# \n\n-   rename_all\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::rename_all(toupper) %>% \n  utils::head(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  PLANT   TYPE  TREATMENT CONC UPTAKE\n1   Qn1 Quebec nonchilled   95     16\n```\n\n\n:::\n:::\n\n\n\n# distinct()\n\n# \n\nA função **distinct()** seleciona apenas as linhas únicas (distintas) de uma determinada variável do banco de dados.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::distinct(Type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Type\n1      Quebec\n2 Mississippi\n```\n\n\n:::\n:::\n\n\n\n# select()\n\n# \n\nA função **select()** permite selecionar colunas nomeando-as e separando-as por vírgula ou não. Veremos exemplos abaixo.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::select(1,3) %>% \n  utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant  Treatment\n1   Qn1 nonchilled\n2   Qn1 nonchilled\n3   Qn1 nonchilled\n```\n\n\n:::\n:::\n\n\n\nVeja que com o código abaixo a saída é a mesma:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>%\n  dplyr::select(Plant, Treatment) %>% \n  utils::head(3)\n```\n:::\n\n\n\n# \n\nVocê pode selecionar colunas utilizando os dois pontos **:**, mas nesse caso, a seleção das colunas seguem a ideia de continuação. Por semplo:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::select(Plant:Treatment) %>% \n  utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant   Type  Treatment\n1   Qn1 Quebec nonchilled\n2   Qn1 Quebec nonchilled\n3   Qn1 Quebec nonchilled\n```\n\n\n:::\n:::\n\n\n\n# \n\nExisem funções no `dplyr` que ajudam na seleção das variáveis de uma outra forma:\n\n-   starts_with(): colunas que começam com um prefixo\n-   ends_with(): colunas que terminam com um sufixo\n-   contains(): colunas que contêm uma string\n-   last_col(): última coluna\n\n# \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::select(starts_with(\"Pl\")) %>% \n  utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant\n1   Qn1\n2   Qn1\n3   Qn1\n```\n\n\n:::\n:::\n\n\n\n# \n\nEsta função nos permite retirar colunas da base da seguinte forma:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::select(-Treatment) %>% \n  utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant   Type conc uptake\n1   Qn1 Quebec   95   16.0\n2   Qn1 Quebec  175   30.4\n3   Qn1 Quebec  250   34.8\n```\n\n\n:::\n:::\n\n\n\n**A função select() também possui as variações select_if(), select_at() e select_all().**\n\n# filter()\n\n# \n\nJá a função **filter()** permite subdividir observações com base em seus valores. O primeiro argumento é o nome do quadro de dados. O segundo argumento e os subsequentes são as expressões que filtram o quadro de dados.\n\n\n\n\n\n\n\nQuando você executa essa linha de código, o pacote `dplyr` executa a operação de filtragem e retorna um novo quadro de dados.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr::filter(dados_Carbon, Treatment == \"nonchilled\") %>% \n  utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant   Type  Treatment conc uptake\n1   Qn1 Quebec nonchilled   95   16.0\n2   Qn1 Quebec nonchilled  175   30.4\n3   Qn1 Quebec nonchilled  250   34.8\n```\n\n\n:::\n:::\n\n\n\n# \n\nVocê pode também utilizar essa função de forma eficaz, você precisa saber como selecionar as observações desejadas usando os operadores de comparação. O R fornece o conjunto padrão: \\>, \\>=, \\<, \\<=, != (diferente) e == (igual).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr::filter(dados_Carbon, conc <= 95) %>% \n  utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant   Type  Treatment conc uptake\n1   Qn1 Quebec nonchilled   95   16.0\n2   Qn2 Quebec nonchilled   95   13.6\n3   Qn3 Quebec nonchilled   95   16.2\n```\n\n\n:::\n:::\n\n\n\n# \n\nVários argumentos do **filter()** são combinados com **“e”**: cada expressão deve ser verdadeira para que uma linha seja incluída na saída. Para outros tipos de combinações, você mesmo precisará usar operadores booleanos: & é “e”, \\| é “ou” e ! é “não”. A Figura abaixo mostra o conjunto completo de operações booleanas.\n\n![](img/boolenaos.jpeg)\n\n# \n\nNo exemplo a seguir utilizamos o operador **\\<=** (menor igual) e o **&**:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::filter(conc <= 95 & Treatment == \"nonchilled\") %>% \n  utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant   Type  Treatment conc uptake\n1   Qn1 Quebec nonchilled   95   16.0\n2   Qn2 Quebec nonchilled   95   13.6\n3   Qn3 Quebec nonchilled   95   16.2\n```\n\n\n:::\n:::\n\n\n\n**A função filter() também possui as variações filter_if(), filter_at() e filter_all().**\n\n# mutate()\n\n# \n\nAlém de selecionar conjuntos de colunas existentes, geralmente é útil adicionar novas colunas que sejam funções de colunas existentes. Esse é o trabalho da função **mutate()**.\n\n**mutate()** sempre adiciona novas colunas no final do seu conjunto de dados, então começaremos criando um conjunto de dados mais restrito para que possamos ver as novas variáveis. Lembre-se que quando você estiver na IDE RStudio, a maneira mais fácil de ver todas as colunas é através da função **View()**.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexemplo_mutate <- dados_Carbon %>% \n                    dplyr::mutate(soma_numeric = conc + uptake,\n                                  plant_trat = stringr::str_c(Plant, Treatment, sep = \"-\")) \n```\n:::\n\n\n\n# \n\nObserve que você pode consultar colunas que acabou de criar:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n dplyr::mutate(soma_numeric = conc + uptake) %>% \n utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant   Type  Treatment conc uptake soma_numeric\n1   Qn1 Quebec nonchilled   95   16.0        111.0\n2   Qn1 Quebec nonchilled  175   30.4        205.4\n3   Qn1 Quebec nonchilled  250   34.8        284.8\n```\n\n\n:::\n:::\n\n\n\n# transmute()\n\n# \n\nSe você quiser apenas manter as novas variáveis, use a função **transmute()**:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexemplo_transmute <- dados_Carbon %>% \n                       dplyr::transmute(soma_numeric = conc + uptake,\n                                     plant_trat = stringr::str_c(Plant, Treatment, sep = \"-\")) %>% \n                       utils::head(3)\n```\n:::\n\n\n\n# arrange()\n\n# \n\nA função **arrange()** orderna linhas.\n\n-   Se a variável for categórica ordena por ordem alfabética\n-   Se a variável for numérica ordena do menor para o maior\n-   Se a varíavel for um fator, ordena pelos níveis do fator\n\n# \n\nO exemplo abaixo ordena o banco de dados de acordo com a variável **uptake**.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>%\n  dplyr::arrange(uptake) %>% \n  utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant        Type Treatment conc uptake\n1   Mc2 Mississippi   chilled   95    7.7\n2   Qc2      Quebec   chilled   95    9.3\n3   Mc1 Mississippi   chilled   95   10.5\n```\n\n\n:::\n:::\n\n\n\n# \n\nMas, nós podemos ordenar as linhas de forma decrescente usando a função **desc()**. Veja o exemplo abaixo.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::arrange(desc(uptake)) %>% \n  utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant   Type  Treatment conc uptake\n1   Qn3 Quebec nonchilled 1000   45.5\n2   Qn2 Quebec nonchilled 1000   44.3\n3   Qn3 Quebec nonchilled  675   43.9\n```\n\n\n:::\n:::\n\n\n\n# \n\nÉ também possível ordenar mais de uma coluna de uma vez\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>%\n  dplyr::arrange(uptake, conc) %>% \n  utils::head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant        Type Treatment conc uptake\n1   Mc2 Mississippi   chilled   95    7.7\n2   Qc2      Quebec   chilled   95    9.3\n3   Mc1 Mississippi   chilled   95   10.5\n```\n\n\n:::\n:::\n\n\n\n# group_by()\n\n# \n\nTemos a função **group_by()** que irá agrupar o banco de dados em relação a uma variável (ou mais de uma).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::group_by(Treatment) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 84 × 5\n# Groups:   Treatment [2]\n   Plant Type   Treatment   conc uptake\n   <ord> <fct>  <fct>      <dbl>  <dbl>\n 1 Qn1   Quebec nonchilled    95   16  \n 2 Qn1   Quebec nonchilled   175   30.4\n 3 Qn1   Quebec nonchilled   250   34.8\n 4 Qn1   Quebec nonchilled   350   37.2\n 5 Qn1   Quebec nonchilled   500   35.3\n 6 Qn1   Quebec nonchilled   675   39.2\n 7 Qn1   Quebec nonchilled  1000   39.7\n 8 Qn2   Quebec nonchilled    95   13.6\n 9 Qn2   Quebec nonchilled   175   27.3\n10 Qn2   Quebec nonchilled   250   37.1\n# ℹ 74 more rows\n```\n\n\n:::\n:::\n\n\n\n#  {background-color=\"black\"}\n\nQuando dados agrupados são passados para uma métrica com reconhecimento de grupo, eles retornarão valores de métrica calculados para cada grupo.\n\n![](img/group.jpeg)\n\n# \n\nLogo, se temos 2 tratamentos, e se quiséssemos calcular a média de cada grupo em relação a variável **conc**, precisaríamos que os dados estivessem agrupados de acordo com o tratamento.\n\n::: fragment\n**A função group_by() é comumente utilizada com a função summarise() que vem logo a seguir**\n:::\n\n# summarise()\n\n# \n\nO último verbo chave é o **summarise()**. Ele recolhe um quadro de dados em uma única linha.\n\nA função **summarise()** não é muito útil, a menos que o combinemos com **group_by()**. Isso altera a unidade de análise do conjunto de dados completo para grupos individuais. Então, quando você usar as funções `dplyr` em um quadro de dados agrupado, eles serão aplicados automaticamente “por grupo”.\n\n# \n\n![](img/summarize-a-variable-in-R.png)\n\n# \n\nPor exemplo, se aplicarmos exatamente o mesmo código a um quadro de dados agrupado por tratamento, obteremos o valor de **uptake** médio por tratamento:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::group_by(Treatment) %>% \n  dplyr::summarise(media_trat = base::mean(uptake, na.rm = TRUE),\n                   n_plantas = dplyr::n())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  Treatment  media_trat n_plantas\n  <fct>           <dbl>     <int>\n1 nonchilled       30.6        42\n2 chilled          23.8        42\n```\n\n\n:::\n:::\n\n\n\n::: fragment\n**Juntos group_by() e summarise() fornecem uma das ferramentas que você usará mais comumente ao trabalhar com `dplyr`: resumos agrupados.**\n:::\n\n# case_when()\n\n# \n\nO **case_when** é uma generalização do **if_else()**. Cada caso é avaliado sequencialmente e a primeira correspondência de cada elemento determina o valor correspondente no vetor de saída. Se nenhum caso corresponder, o **.default** será usado como uma instrução final \"else\".\n\n-   case_when() é um equivalente em R da CASE WHEN instrução `SQL` \"pesquisada\".\n\n# \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon %>% \n  dplyr::mutate(\n    new_plant = dplyr::case_when(conc >= 125 ~ \"Tipo1\",\n                                 .default = \"Tipo2\")\n    ) %>% \n  utils::head(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Plant   Type  Treatment conc uptake new_plant\n1   Qn1 Quebec nonchilled   95     16     Tipo2\n```\n\n\n:::\n:::\n\n\n\n# across()\n\n**across()** facilita a aplicação da mesma transformação a múltiplas colunas, permitindo que você use a semântica **select()** dentro de funções de \"mascaramento de dados\" como **summarise()** e **mutate()**.\n\n**if_any()** e **if_all()** aplicam a mesma função de predicado a uma seleção de colunas e combinam os resultados em um único vetor lógico.\n\n# \n\n-   if_any() é TRUE quando o predicado é TRUE para qualquer uma das colunas selecionadas\n-   if_all() é TRUE quando o predicado é TRUE para todas as colunas selecionadas\n\nSe você só precisa selecionar colunas sem aplicar uma transformação a cada uma delas, provavelmente desejará usar **pick()**.\n\n::: fragment\n**across() substitui a família de \"variantes com escopo definido\" como summarise_at(), summarise_if() e summarise_all().**\n:::\n\n# \n\nUm exemplo bem rapidinho:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon%>%   \n  dplyr::group_by(Treatment) %>%   \n  dplyr::summarise(dplyr::across(c(conc,uptake), mean, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  Treatment   conc uptake\n  <fct>      <dbl>  <dbl>\n1 nonchilled   435   30.6\n2 chilled      435   23.8\n```\n\n\n:::\n:::\n\n\n\n::: fragment\nDeixo o post no meu blog sobre [a função across](https://www.fernandakellyrs.com/post/fun%C3%A7%C3%A3o-across) para vocês estudarem mais sobre ela.\n:::\n\n#  {background-image=\"img/esquisse.jpeg\"}\n\n# \n\nO esquisse é um pacote que permite a criação de gráficos em {ggplot2} de maneira point and click, o que torna a construção de gráficos uma tarefa bem mais simples, pois não há necessidade de digitar as linhas de código.\n\nAlém disso, é possível recuperar o código que gerou os gráficos, o que é ótimo para garantir a reprodutibilidade dos resultados!\n\n# Legal, né?\n\n# \n\nVamos brincar um pouco com essa aplicação.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(esquisse)\nesquisse::esquisser(viewer = \"browser\")\n```\n:::\n\n\n\n\n#  {background-image=\"img/teste_hipo.png\"}\n\n##\n\n**A parte final de uma análise estatı́stica, o teste de hipótese (T.H.), se dá pela tomada de decisão relativa a um problema de interesse, digamos, verificar se há associação entre um fator (adubo) e o tamanho de uma planta (mangueira) na experimentação agronômica.**\n\nNa prática experimental é comum o surgimento de situações em que não se tem o conhecimento da distribuição dos dados, ou mesmo que o tenha, os mesmos podem não ter a distribuição normal. A alta dispersão ou até mesmo assimetria, ou ambos, fazem com que o trato com os dados seja dificultado.\n\n::: {.notes}\n-   **Muitos casos destes existem técnicas de transformações nos dados que produzem novos dados cuja distribuição se aproxime da normal**. Caso isto não seja possı́vel as técnicas não-paramétricas são uma alternativa para análise estatı́stica.\n:::\n\n# Paramétricos\n\nHá três principais tipos de testes t: o teste t para uma amostra, o teste t para duas amostras independentes (ou apenas teste t independente) e o teste t para duas amostras pareadas (ou teste t pareado). O teste t é um teste que verifica se há **diferença média** entre as amostras.\n\n::: fragment\nE se você tiver mais de duas amostras ou grupos?\n:::\n\n# \n\nA resposta mais inocente seria realizar múltiplos testes t entre os diversos grupos e comparar o tamanho de efeito e p-valor dos testes. Porém essa abordagem possui uma falha: conforme aumenta o número de testes a taxa de falsos positivos (erros tipo I) aumentam quase na mesma proporção. **Esse erro ocorre quando rejeitamos a hipótese nula quando de fato ela é verdadeira.** \n\n# \n\nPara comparação entre mais de duas amostras ou grupos, temos um conjunto de técnicas chamada análise de variância, conhecida como **Analysis of Variance (ANOVA)**.\n\nA ANOVA controla esses erros para que os falsos positivos (erros tipo I) permaneçam em 5% na comparação de média entre dois ou mais grupos. **A ANOVA não irá dizer com precisão como que as médias dos grupos diferem**, mas o seu resultado indica fortes evidências de que a diferença entre as médias dos grupos difere. Sua hipótese nula é que não há diferença entre as médias dos grupos.\n\n# Vamos de exemplo?\n\n# \n\nNós vimos que há uma suposição entre a média do vetor numérico de taxas de absorção de dióxido de carbono se difere entre os tratamentos. Sendo assim, vamos levantar a seguinte hipótese entre os grupos nonchilled e chilled:\n\n- $H_{0}$ : Há uma diferença média entreos grupos nonchilled e chilled em relação as taxas de absorção de dióxido de carbono\n\n## Pressupostos\n\n- Normalidade: verificamos a normalidade da variável dependente de cada grupo;\n- Homogeneidade de variâncias (ou seja, grupos com variâncias homogêneas).\n\n## Normalidade\n\nPara verificar a normalidade dos dados, é recomendável utilizar o teste de **Shapiro WIlk** quando o número de amostra é menor que 50. A estatı́stica T é basicamente o quadrado de um coeficiente de correlação, onde o coeficiente de correlação de Pearson é calculado entre a estatı́stica ordenada X i na amostra e o escore ai , que representa o que a estatı́stica ordenada deveria parecer se a população é Normal. Portanto, as hipóteses levantadas são:\n\n- $H_{0}$ : F(x) é função de distribuição normal;\n- $H_{1}$ : F(x) não é função de distribuição normal;\n\n#\n\nIremos fixar o nível de 95% de significância.\n\nA estatística de teste já foi mencionada acima, mas vale frizar que vamos utilizar o teste de **Shapiro WIlk** .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstatix)\n\nteste_shapiro <- dados_Carbon %>% \n                  dplyr::group_by(Treatment) %>% \n                  rstatix::shapiro_test(uptake)\n\nknitr::kable(teste_shapiro)\n```\n\n::: {.cell-output-display}\n\n\n|Treatment  |variable | statistic|         p|\n|:----------|:--------|---------:|---------:|\n|nonchilled |uptake   | 0.9450490| 0.0430199|\n|chilled    |uptake   | 0.8978853| 0.0012451|\n\n\n:::\n:::\n\n\n\n#\n\nVeja que de acordo com o teste de Shapiro Wilk, ao nível de 95% de significância, **rejeita-se a hipótese nula**, ou seja, a variável **uptake** não é descrita por uma distribuição normal.\n\n# E agora, faço o quê?\n\n# Transformação de Box-Cox\n\nEm estatística, uma transformação de potência é uma família de funções que são aplicadas para criar a transformação monotônica de dados usando funções de potência. Esta é uma técnica de transformação de dados útil usada para estabilizar a variância, tornar os dados mais semelhantes à distribuição normal, melhorar a validade das medidas de associação (como a correlação de Pearson entre as variáveis) e para outros procedimentos de estabilização de dados.\n\n#\n\n**A transformação de Box-Cox recebeu o nome dos estatísticos que a formularam, George E. P. Box y David Cox, em artigo de 1964 (“An Analysis of Transformations”).**\n\n#\n\nEssa transformação consiste em transformar os dados de aordo com a seguinte expressão:\n\n- $y' = \\frac{y^\\lambda - 1}{\\lambda}$\n\nonde $\\lambda$ é um parâmetro a ser estimado dos dados. Se $\\lambda = 0$, temos:\n\n- $y' = ln(y)$\n\n**Então a nossa missão é encontrar o valor de $\\lambda$.**\n\n#\n\nNós vamos utilizar o pacote `MASS` para obter o valor de interesse para executar a transformação. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda <- MASS::boxcox(aov(uptake+0.000001~Treatment,  data = dados_Carbon ))\n```\n\n::: {.cell-output-display}\n![](Slides_files/figure-revealjs/unnamed-chunk-33-1.png){width=960}\n:::\n\n```{.r .cell-code}\nlambda$x[which.max(lambda$y)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8282828\n```\n\n\n:::\n:::\n\n\n\n#\n\nDesta forma o próximo passo é obter os dados transformados e depois realizar as análises utilizando estes novos dados.\n\nNós aprendemos a função **mutate** e agora vamos utilizá-la para criar essa nova variável.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados_Carbon <- dados_Carbon %>% \n                  dplyr::mutate(new_uptake = ((uptake^0.8282828) - 1)/0.8282828)\n```\n:::\n\n\n\n::: fragment\nVamos plotar essa nova variável?\n:::\n\n\n#\n\nO resultado nos demonstra que, novamente, a variável de interesse não é descrita pela distribuição normal ao nível de 95% de significância.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstatix)\n\nteste_shapiro2 <- dados_Carbon %>% \n                  dplyr::group_by(Treatment) %>% \n                  rstatix::shapiro_test(new_uptake)\n\nknitr::kable(teste_shapiro2)\n```\n\n::: {.cell-output-display}\n\n\n|Treatment  |variable   | statistic|         p|\n|:----------|:----------|---------:|---------:|\n|nonchilled |new_uptake | 0.9362075| 0.0210991|\n|chilled    |new_uptake | 0.9067580| 0.0023080|\n\n\n:::\n:::\n\n\n\n\n# Eita! E agora?\n\n#\n\nNa prática experimental é comum o surgimento de situações em que não se tem o conhecimento da distribuição dos dados, ou mesmo que o tenha, os mesmos podem não ter a distribuição normal. A alta dispersão ou até mesmo assimetria, ou ambos, fazem com que o trato com os dados seja dificultado.\n\nMuitos casos destes existem técnicas de transformações nos dados que produzem novos dados cuja distribuição se aproxime da normal. Caso isto não seja possı́vel as técnicas não-paramétricas são uma alternativa para análise estatı́stica.\n\nUm teste não-paramétrico é aquele cujo modelo não especifica condições sobre os parâmetros da população da qual a amostra foi obtida.\n\n\n# E porquê eu usaria esse método?\n\n#\n\n1. São menos exigentes, restritivos, do que os testes paramétricos, pois dispensam a normalidade dos dados;\n\n::: fragment\n2. Em geral, as probabilidades das afirmativas são probabilidades exatas, salvo que se usam algumas aproximações para grandes amostras;\n:::\n\n::: fragment\n3. Independem da forma da população da qual a amostra foi obtida;\n:::\n\n::: fragment\n4. Os cálculos são relativamente fáceis;\n:::\n\n#\n\n5. Alguns testes não-paramétricos permitem trabalhar com dados de diferentes populações, ao contrário dos paramétricos;\n\n::: fragment\n6. São úteis nos casos em que é difı́cil estabelecer uma escala de valores quantitativos para os dados, por exemplo, escalas ordinais quantitativas (de ansiedade - psiquiatria) ou escalas ordinais qualitativas (ruim-bom-ótimo);\n:::\n\n::: fragment\n7. São mais eficientes que os paramétricos quando não há pressuposição de normalidade - Os testes paramétricos são mais poderosos se os pressupostos são verificados.\n:::\n\n#\n\nOs teste não paramétricos não serão o nosso foco aqui, mas deixo o nome de cada um e sua equivalência para que vocês estudem futuramente:\n\n- Mann-Whitney\n- Wilcoxon\n- Kruskal-Wallis\n- Teste de Dunn  (Procedimento de Bonferroni)\n\n# Trocando de banco de dados...\n\n# Banco de dados Iris\n\nEste famoso conjunto de dados de íris (de Fisher ou Anderson) fornece as medidas em centímetros das variáveis comprimento e largura da sépala e comprimento e largura da pétala, respectivamente, para 50 flores de cada uma das 3 espécies de íris. As espécies são Iris setosa, versicolor e virginica.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(datasets)\ndados_iris <- datasets::iris \n```\n:::\n\n\n\n## Homogeneidade de variâncias\n\nO segundo pressuposto pode ser avaliado através do  **teste de Levene**, em que as hipóteses são:\n\n- $H_{0}$ : Os grupos apresentam variâncias homogêneas;\n- $H_{1}$ : Os grupos não apresentam variâncias homogêneas;\n\nVamos considerar o nível de 95% de significância.\n\n#\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nteste_levene <- dados_iris %>% \n                  dplyr::filter(Species != \"versicolor\") %>% \n                  dplyr::group_by(Species) \n\nteste_levene_Resultado <- car::leveneTest(Sepal.Length ~ Species,\n                                          data = teste_levene, \n                                          center = mean)\nknitr::kable(teste_levene_Resultado)\n```\n\n::: {.cell-output-display}\n\n\n|      | Df|  F value|    Pr(>F)|\n|:-----|--:|--------:|---------:|\n|group |  1| 13.70568| 0.0003534|\n|      | 98|       NA|        NA|\n\n\n:::\n:::\n\n\n\n#\n\nLogo, ao nível de 95% de significância, **rejeitamos a hipótese nula** (p-value = 0.0003), ou seja, os grupos apresentam não variâncias homogêneas e assim não podemos seguir com o teste t independente.\n\n::: fragment\n**Violamos o pressuposto de variâncias homogêneas, e agora?**\n:::\n\n::: fragment\nNós poderemos seguir com o teste-t, mas deveremos usar um teste-t com correção para essa violação da homogeneidade, a **correção de Welch**.\n:::\n\n# Partiu teste, Fê?\n\n# ...se liga só no teste e em suas miniciosidades.\n\n#\n\nComo violamos o teste de homogeinidade de variâncias, vamos programar com **var.equal=FALSE**, mas uma vez que o pressuposto da homogeneidade de variâncias foi atendido, colocaremos **var.equal=TRUE**.\n\n::: fragment\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::t.test(Sepal.Length ~ Species,\n              data = teste_levene,\n              var.equal = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  Sepal.Length by Species\nt = -15.386, df = 76.516, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group setosa and group virginica is not equal to 0\n95 percent confidence interval:\n -1.78676 -1.37724\nsample estimates:\n   mean in group setosa mean in group virginica \n                  5.006                   6.588 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n#\n\nQual é a interpretação dessa saída?\n\n::: fragment\n**Ao nível de 95% de significância, nós rejeitamos a $H_{0}$ e assim há diferença média entre as Species e o tamanho da Sepal.Length.**\n:::\n\n::: fragment\nSe o nosso interesse for comparar os três tipos de Species, devemos utilizar a ANOVA e, para um melhor entendimento da diferença, utilizariamos o teste de Tukey para apontar quais grupos são diferentes entre si.\n:::\n\n#\n\nMas antes de tudo...\n\n# Teste t\n\n## paired = TRUE\n\nEm algumas situações o par de amostras é constituı́do do mesmo indivı́duo em duas ocasiões diferentes. Essa ocasiação nós denominamos por **pareamento**, em que neste pareamento podemos distingui-lo em três tipos: auto-pareamento, pareamento natural e pareamento artificial.\n\n#\n\n- O auto-pareamento ocorre quando o indivı́duo serve como seu próprio controle, como na situação\nem que um indivı́duo recebe duas drogas administradas em ocasiões diferentes. Outra situação é\na que um tratamento é administrado e as variáveis de interesse são observadas antes e depois do\nprograma. Finalmente, a comparação de dois órgãos no mesmo indivı́duo, como braços, pernas,\nolhos, narinas, segundo alguma caracterı́stica estudada também constitui um auto-pareamento.\n\n#\n\n- O pareamento natural consiste em formar pares tão homogêneos quanto possı́vel, controlando\nos fatores que possam interferir na resposta, sendo que o pareamento aparece de forma natural. Por\nexemplo, em experimentos de laboratório pode-se formar pares de cobaias selecionadas da mesma\nninhada; em investigações clı́nicas, gêmeos univitelinos são muito usados.\n\n- No pareamento artificial escolhe-se indivı́duos com caracterı́sticas semelhantes, tais como, idade,\nsexo, nı́vel sócio-econômico, estado de saúde ou, em geral, fatores que podem influenciar de maneira\nrelevante a variável resposta.\n\n#\n\n```r\nt.test(x, ...)\n\n## Default S3 method:\nt.test(x, y = NULL,\n       alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0,\n       **paired = FALSE**,\n       var.equal = FALSE,\n       conf.level = 0.95, ...)\n\n## S3 method for class 'formula'\nt.test(formula, data, subset, na.action, ...)\n```\n\n## alternative = c(\"two.sided\", \"less\", \"greater\")\n\nA hipótese alternativa mais comum - e a mais recomendada, salvo raras exceções - é a de que há uma diferença entre os grupos. Observe que aqui estamos estabelecendo apenas que, caso os grupos não sejam iguais, eles serão diferentes. Não estamos nos comprometendo com o sentido dessa diferença, se o grupo A terá uma média superior ou inferior à do grupo B. No entanto, é possível construirmos hipóteses alternativas que estabeleçam o sentido dessa diferença:\n\n#\n\n- $H_{1}$ : média do grupo A > média do grupo B\n- $H_{1}$ : média do grupo A < média do grupo B\n\n\n![](img/bilateral.png)\n\n::: footer\nArte por [Allison Horst](https://mobile.twitter.com/allison_horst)\n:::\n\n#\n\n```r\nt.test(x, ...)\n\n## Default S3 method:\nt.test(x, y = NULL,\n       **alternative = c(\"two.sided\", \"less\", \"greater\")**,\n       mu = 0,\n       paired = FALSE,\n       var.equal = FALSE,\n       conf.level = 0.95, ...)\n\n## S3 method for class 'formula'\nt.test(formula, data, subset, na.action, ...)\n```\n\n# ANOVA\n\n# ... bem rapidinho, pode ser?\n\n#\n\n::: fragment\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova <- stats::aov(Sepal.Length ~ Species, dados_iris)\nsummary(anova)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Df Sum Sq Mean Sq F value Pr(>F)    \nSpecies       2  63.21  31.606   119.3 <2e-16 ***\nResiduals   147  38.96   0.265                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n:::\n\n::: fragment\nQual é a interpretação desse teste?\n:::\n\n#\n\n::: fragment\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstats::TukeyHSD(anova)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: stats::aov(formula = Sepal.Length ~ Species, data = dados_iris)\n\n$Species\n                      diff       lwr       upr p adj\nversicolor-setosa    0.930 0.6862273 1.1737727     0\nvirginica-setosa     1.582 1.3382273 1.8257727     0\nvirginica-versicolor 0.652 0.4082273 0.8957727     0\n```\n\n\n:::\n:::\n\n\n:::\n\n::: fragment\nOnde há diferença?\n:::\n\n\n# K-Means\n\nTá aí um algoritmo muito falado na área de ciência de dados e, apesar de estar na 'boca do povo' há uns 5 anos, seus princípios são bem mais velhos do que isso. \n\nO K-Means é um algoritmo de aprendizagem não supervisionada. Ele não classifica, mas agrupa vetores de atributos similares, isto é, coloca em um mesmo agrupamento vetores similares, e por este ser computacionalmente simples e funcionar bem na prática, ele é um dos principais e mais usados métodos de agrupamento (DUDA; HART, 1973).  \n\nSó no Google Acadêmico com a pesquisa \"K-means\" há mais de 8 mil artigos publicados. São artigos com aplicação em diversas áreas e claro que muitos são artigos 'palpiteros' que indicam algum tipo de alteração no desenrolar do algoritmo. \n\n#\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(dados_iris, aes(Petal.Length, Petal.Width)) + \n  geom_point(aes(col=Species), size=4)\n```\n\n::: {.cell-output-display}\n![](Slides_files/figure-revealjs/unnamed-chunk-41-1.png){width=960}\n:::\n:::\n\n\n\n\n\n# Muita informação, né?\n\n# ... chegamos ao fim!\n\n#\n\nTodo o material vocês encontram [aqui](https://github.com/FernandaKelly) e, se tiverem dúvidas, podem entrar em contato comigo através do meu [site](https://www.fernandakellyrs.com/) ou [linkedin](https://www.linkedin.com/in/fernandakellyrs/).\n\n\n",
    "supporting": [
      "Slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}